# ðŸ“˜ Scenario 31: Node Clock Drift Causing Authentication Failures

**Category**: Cluster Authentication  
**Environment**: Kubernetes 1.22, On-prem, kubeadm  
**Impact**: Cluster-wide authentication failures lasting 2+ hours  

---

## Scenario Summary  
Clock drift exceeding 5 minutes between worker nodes and control plane caused JWT token validation failures, breaking all token-based authentication across the cluster.

---

## What Happened  
- **Time synchronization failure**:  
  - NTP daemon disabled during OS patching  
  - Worker nodes drifted 7-12 minutes over 3 weeks  
- **Authentication symptoms**:  
  - `kubectl` commands failed with `Token cannot be validated: clock skew too great`  
  - Pod-to-API communication errors (`x509: certificate has expired or is not yet valid`)  
  - Service accounts unable to refresh tokens  
- **Root discovery**:  
  - Control plane at UTC-5, nodes at UTC+7  
  - `kube-apiserver` logs showed `too much clock skew` warnings  

---

## Diagnosis Steps  

### 1. Verify authentication errors:
```sh
kubectl get pods -A 2>&1 | grep -i "clock skew"
# Output: "Token cannot be validated: clock skew of 12m exceeds 5m"
```

### 2. Check node times:
```sh
kubectl get nodes -o json | jq -r '.items[].metadata.name' | \
  xargs -I{} sh -c 'echo {}; kubectl debug node/{} -it --image=busybox -- date -u'
# Showed 5-15 minute drift
```
