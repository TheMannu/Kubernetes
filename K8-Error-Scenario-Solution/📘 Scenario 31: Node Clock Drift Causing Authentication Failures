# üìò Scenario 31: Node Clock Drift Causing Authentication Failures

**Category**: Cluster Authentication  
**Environment**: Kubernetes 1.22, On-prem, kubeadm  
**Impact**: Cluster-wide authentication failures lasting 2+ hours  

---

## Scenario Summary  
Clock drift exceeding 5 minutes between worker nodes and control plane caused JWT token validation failures, breaking all token-based authentication across the cluster.

---

## What Happened  
- **Time synchronization failure**:  
  - NTP daemon disabled during OS patching  
  - Worker nodes drifted 7-12 minutes over 3 weeks  
- **Authentication symptoms**:  
  - `kubectl` commands failed with `Token cannot be validated: clock skew too great`  
  - Pod-to-API communication errors (`x509: certificate has expired or is not yet valid`)  
  - Service accounts unable to refresh tokens  
- **Root discovery**:  
  - Control plane at UTC-5, nodes at UTC+7  
  - `kube-apiserver` logs showed `too much clock skew` warnings  

---

## Diagnosis Steps  

### 1. Verify authentication errors:
```sh
kubectl get pods -A 2>&1 | grep -i "clock skew"
# Output: "Token cannot be validated: clock skew of 12m exceeds 5m"
```

### 2. Check node times:
```sh
kubectl get nodes -o json | jq -r '.items[].metadata.name' | \
  xargs -I{} sh -c 'echo {}; kubectl debug node/{} -it --image=busybox -- date -u'
# Showed 5-15 minute drift
```

### 3. Inspect time services:
```sh
kubectl debug node/<node> -it --image=ubuntu -- systemctl status chrony
# Output: inactive (dead)
```

### 4. Verify token validity:
```sh
kubectl describe secret -n kube-system $(kubectl get secret -n kube-system | \
  grep service-account-token | head -1 | awk '{print $1}') | \
  grep "Expires:"
# Output: Expires: 2023-05-15 14:22:00 +0000 UTC (conflicted with node time)
```

---

## Root Cause  
**Time synchronization breakdown**:  
1. NTP service disabled on worker nodes  
2. No time drift monitoring in place  
3. Kubernetes' strict 5-minute clock skew tolerance  

---

## Fix/Workaround  

### Immediate Recovery:
```sh
# 1. Force time sync on all nodes
ansible all -i hosts -m shell -a "chronyc makestep && systemctl restart chronyd"

# 2. Verify synchronization
kubectl get nodes -o json | jq -r '.items[].metadata.name' | \
  xargs -I{} kubectl debug node/{} -it --image=ubuntu -- \
  chronyc tracking | grep -i "last offset"
```

### Long-term Solution:
```yaml
# Node bootstrap configuration
apiVersion: kubeadm.k8s.io/v1beta3
kind: JoinConfiguration
nodeRegistration:
  kubeletExtraArgs:
    system-reserved: "chronyd=100m"
  taints: null
  ignorePreflightErrors: null
```

---

## Lessons Learned  
‚ö†Ô∏è **Time is critical infrastructure**: Microservices depend on synchronized clocks  
‚ö†Ô∏è **Silent degradation**: Drift accumulates unnoticed until auth breaks  
‚ö†Ô∏è **Kubernetes is unforgiving**: 5-minute tolerance is non-negotiable  

---

## Prevention Framework  

### 1. Time Synchronization Hardening
```yaml
# Machine config for chrony
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 50-worker-chrony
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - contents: |
          [Unit]
          Description=NTP client/server
          After=network.target
          [Service]
          Type=simple
          ExecStart=/usr/sbin/chronyd -n -F 1
          Restart=always
          [Install]
          WantedBy=multi-user.target
        enabled: true
        name: chronyd.service
```

