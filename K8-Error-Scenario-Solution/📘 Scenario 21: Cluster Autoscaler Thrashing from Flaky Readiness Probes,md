# ðŸ“˜ Scenario 21: Cluster Autoscaler Thrashing from Flaky Readiness Probes

**Category**: Cluster Scaling  
**Environment**: Kubernetes v1.24, AWS EKS with Cluster Autoscaler v1.22.2  
**Impact**: 40% cost overrun from node churn, workload instability  

---

## Scenario Summary  
A deployment with an unreliable readiness probe triggered continuous node scaling cycles (5-7 per hour), causing performance degradation and cloud cost spikes.

---

## What Happened  
- **Problematic deployment**:  
  - Readiness probe checked an endpoint with 30% failure rate  
  - `kubectl get pods` showed pods alternating `Ready/NotReady`  
- **Autoscaler reaction**:  
  - Scaled up when >3 pods were `NotReady` (considered unschedulable)  
  - Scaled down 10 minutes after pods recovered  
- **Observed symptoms**:  
  - AWS EC2 `InstanceLaunchRate` exceeded account limits  
  - Node `Ready` duration histogram showed 80% <15 minutes  
  - CloudWatch showed `CPUUtilization` sawtooth pattern  

---

## Diagnosis Steps  

### 1. Identify scaling loops:
```sh
kubectl -n kube-system logs -l app=cluster-autoscaler --tail=100 | \
  grep -E "ScaleUp|ScaleDown"
# Output showed 6 scale-up/scale-down cycles in 2 hours
```

### 2. Locate problematic pods:
```sh
kubectl get events --sort-by=.lastTimestamp | \
  grep -i "probe failed"
# Revealed deployment/frontend pods failing probes
```
