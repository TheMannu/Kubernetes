# 📘 Scenario 21: Cluster Autoscaler Thrashing from Flaky Readiness Probes

**Category**: Cluster Scaling  
**Environment**: Kubernetes v1.24, AWS EKS with Cluster Autoscaler v1.22.2  
**Impact**: 40% cost overrun from node churn, workload instability  

---

## Scenario Summary  
A deployment with an unreliable readiness probe triggered continuous node scaling cycles (5-7 per hour), causing performance degradation and cloud cost spikes.

---

## What Happened  
- **Problematic deployment**:  
  - Readiness probe checked an endpoint with 30% failure rate  
  - `kubectl get pods` showed pods alternating `Ready/NotReady`  
- **Autoscaler reaction**:  
  - Scaled up when >3 pods were `NotReady` (considered unschedulable)  
  - Scaled down 10 minutes after pods recovered  
- **Observed symptoms**:  
  - AWS EC2 `InstanceLaunchRate` exceeded account limits  
  - Node `Ready` duration histogram showed 80% <15 minutes  
  - CloudWatch showed `CPUUtilization` sawtooth pattern  

---

## Diagnosis Steps  

### 1. Identify scaling loops:
```sh
kubectl -n kube-system logs -l app=cluster-autoscaler --tail=100 | \
  grep -E "ScaleUp|ScaleDown"
# Output showed 6 scale-up/scale-down cycles in 2 hours
```

### 2. Locate problematic pods:
```sh
kubectl get events --sort-by=.lastTimestamp | \
  grep -i "probe failed"
# Revealed deployment/frontend pods failing probes
```

### 3. Analyze probe configuration:
```sh
kubectl get deploy frontend -o yaml | yq '.spec.template.spec.containers[].readinessProbe'
# Showed HTTP probe to /health with 1s timeout
```

### 4. Verify scaling triggers:
```sh
kubectl -n kube-system describe configmap cluster-autoscaler-status | \
  grep -A5 "ScaleUp"
```

---

## Root Cause  
**Probe misalignment**:  
1. Readiness endpoint had 300ms latency spikes (exceeding 1s timeout)  
2. No retries configured (`failureThreshold: 1`)  
3. Autoscaler reacted faster than pod stabilization  

---

## Fix/Workaround  

### Immediate Stabilization:
```sh
# 1. Adjust autoscaler cooldowns
kubectl -n kube-system edit deploy cluster-autoscaler
# Add:
# --scale-down-delay-after-add=20m
# --scale-down-unneeded-time=20m

# 2. Patch problematic deployment
kubectl patch deploy frontend -p '{
  "spec":{
    "template":{
      "spec":{
        "containers":[{
          "name":"app",
          "readinessProbe":{
            "timeoutSeconds":3,
            "periodSeconds":5,
            "failureThreshold":3
          }
        }]
      }
    }
  }
}'
```

### Long-term Solution:
```yaml
# Autoscaler Helm values
autoDiscovery:
  clusterName: my-cluster
extraArgs:
  balance-similar-node-groups: true
  skip-nodes-with-system-pods: false
  scale-down-delay-after-add: 15m
  scale-down-unneeded-time: 15m
  max-node-provision-time: 10m
```

---

## Lessons Learned  
⚠️ **Probes are scaling triggers**: Flakiness causes infrastructure ripple effects  
⚠️ **Cooldowns matter**: Autoscaler defaults may be too aggressive  
⚠️ **Monitoring gaps**: Need visibility into probe failures vs scaling  

---
