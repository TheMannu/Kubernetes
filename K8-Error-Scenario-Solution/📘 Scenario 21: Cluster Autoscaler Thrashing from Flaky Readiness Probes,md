# 📘 Scenario 21: Cluster Autoscaler Thrashing from Flaky Readiness Probes

**Category**: Cluster Scaling  
**Environment**: Kubernetes v1.24, AWS EKS with Cluster Autoscaler v1.22.2  
**Impact**: 40% cost overrun from node churn, workload instability  

---

## Scenario Summary  
A deployment with an unreliable readiness probe triggered continuous node scaling cycles (5-7 per hour), causing performance degradation and cloud cost spikes.

---

## What Happened  
- **Problematic deployment**:  
  - Readiness probe checked an endpoint with 30% failure rate  
  - `kubectl get pods` showed pods alternating `Ready/NotReady`  
- **Autoscaler reaction**:  
  - Scaled up when >3 pods were `NotReady` (considered unschedulable)  
  - Scaled down 10 minutes after pods recovered  
- **Observed symptoms**:  
  - AWS EC2 `InstanceLaunchRate` exceeded account limits  
  - Node `Ready` duration histogram showed 80% <15 minutes  
  - CloudWatch showed `CPUUtilization` sawtooth pattern  

---

## Diagnosis Steps  

### 1. Identify scaling loops:
```sh
kubectl -n kube-system logs -l app=cluster-autoscaler --tail=100 | \
  grep -E "ScaleUp|ScaleDown"
# Output showed 6 scale-up/scale-down cycles in 2 hours
```

### 2. Locate problematic pods:
```sh
kubectl get events --sort-by=.lastTimestamp | \
  grep -i "probe failed"
# Revealed deployment/frontend pods failing probes
```

### 3. Analyze probe configuration:
```sh
kubectl get deploy frontend -o yaml | yq '.spec.template.spec.containers[].readinessProbe'
# Showed HTTP probe to /health with 1s timeout
```

### 4. Verify scaling triggers:
```sh
kubectl -n kube-system describe configmap cluster-autoscaler-status | \
  grep -A5 "ScaleUp"
```

---

## Root Cause  
**Probe misalignment**:  
1. Readiness endpoint had 300ms latency spikes (exceeding 1s timeout)  
2. No retries configured (`failureThreshold: 1`)  
3. Autoscaler reacted faster than pod stabilization  

---

## Fix/Workaround  

### Immediate Stabilization:
```sh
# 1. Adjust autoscaler cooldowns
kubectl -n kube-system edit deploy cluster-autoscaler
# Add:
# --scale-down-delay-after-add=20m
# --scale-down-unneeded-time=20m

# 2. Patch problematic deployment
kubectl patch deploy frontend -p '{
  "spec":{
    "template":{
      "spec":{
        "containers":[{
          "name":"app",
          "readinessProbe":{
            "timeoutSeconds":3,
            "periodSeconds":5,
            "failureThreshold":3
          }
        }]
      }
    }
  }
}'
```

### Long-term Solution:
```yaml
# Autoscaler Helm values
autoDiscovery:
  clusterName: my-cluster
extraArgs:
  balance-similar-node-groups: true
  skip-nodes-with-system-pods: false
  scale-down-delay-after-add: 15m
  scale-down-unneeded-time: 15m
  max-node-provision-time: 10m
```

---

## Lessons Learned  
⚠️ **Probes are scaling triggers**: Flakiness causes infrastructure ripple effects  
⚠️ **Cooldowns matter**: Autoscaler defaults may be too aggressive  
⚠️ **Monitoring gaps**: Need visibility into probe failures vs scaling  

---

## Prevention Framework  

### 1. Probe Validation Checklist
```markdown
1. [ ] Test under peak load (latency >3x normal)  
2. [ ] Set `timeoutSeconds` > P99 endpoint latency  
3. [ ] Configure `failureThreshold` >=3  
4. [ ] Verify with `kubectl get --raw="/readyz?verbose"`  
```

### 2. Autoscaler Safeguards
```yaml
# Prometheus alerts
- alert: RapidNodeChurn
  expr: increase(cluster_autoscaler_scale_ups_total[1h]) > 5
  labels:
    severity: critical
  annotations:
    summary: "Cluster scaling thrashing ({{ $value }} scale-ups/hour)"

- alert: FailedProbesHigh
  expr: sum(rate(kubelet_prober_probe_total{result!="success"}[5m])) by (pod, probe_type) > 5
  labels:
    severity: warning
```

### 3. Deployment Policies
```yaml
# OPA/Gatekeeper constraint
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sValidReadinessProbes
metadata:
  name: readiness-probe-timeout-check
spec:
  match:
    kinds:
    - apiGroups: ["apps"]
      kinds: ["Deployment", "StatefulSet"]
  parameters:
    minTimeoutSeconds: 2
    minFailureThreshold: 3
```

### 4. Load Testing
```sh
# Simulate traffic spikes
kubectl run -it --rm vegeta --image=peterevans/vegeta -- \
  sh -c "echo 'GET http://frontend:8080/health' | vegeta attack -rate=100/s -duration=1m | vegeta report"
```

---
