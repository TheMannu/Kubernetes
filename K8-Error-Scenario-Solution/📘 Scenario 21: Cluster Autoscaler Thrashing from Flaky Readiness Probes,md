# ðŸ“˜ Scenario 21: Cluster Autoscaler Thrashing from Flaky Readiness Probes

**Category**: Cluster Scaling  
**Environment**: Kubernetes v1.24, AWS EKS with Cluster Autoscaler v1.22.2  
**Impact**: 40% cost overrun from node churn, workload instability  

---

## Scenario Summary  
A deployment with an unreliable readiness probe triggered continuous node scaling cycles (5-7 per hour), causing performance degradation and cloud cost spikes.

---

## What Happened  
- **Problematic deployment**:  
  - Readiness probe checked an endpoint with 30% failure rate  
  - `kubectl get pods` showed pods alternating `Ready/NotReady`  
- **Autoscaler reaction**:  
  - Scaled up when >3 pods were `NotReady` (considered unschedulable)  
  - Scaled down 10 minutes after pods recovered  
- **Observed symptoms**:  
  - AWS EC2 `InstanceLaunchRate` exceeded account limits  
  - Node `Ready` duration histogram showed 80% <15 minutes  
  - CloudWatch showed `CPUUtilization` sawtooth pattern  

---